---
format: html
---

# Boston Housing Dataset - Modules & Packages Demo

## Learning Objectives
- Organize data analysis code into reusable modules
- Create a Python package for data preprocessing
- Practice importing and reloading modules
- Apply type hints to functions
- Build a practical data science toolkit

---

## Part 1: Starting Point - Messy Notebook Code

Let's start with code that works but is disorganized (similar to what we did in the main exercise):

```python
# All code in one notebook - hard to reuse!
import math
from typing import Dict, List

# Load data
def load_csv(filename: str) -> Dict[str, List[float]]:
    data = {}
    with open(filename, 'r') as file:
        header = file.readline().strip()
        columns = header.split(',')
        for col in columns:
            data[col] = []
        for line in file:
            values = line.strip().split(',')
            for col, value in zip(columns, values):
                data[col].append(float(value))
    return data

# Statistics
def mean(values: List[float]) -> float:
    return sum(values) / len(values)

def std(values: List[float]) -> float:
    n = len(values)
    avg = mean(values)
    variance = sum((x - avg) ** 2 for x in values) / (n - 1)
    return math.sqrt(variance)

# Scaling
def min_max_scale(values: List[float]) -> List[float]:
    min_val = min(values)
    max_val = max(values)
    range_val = max_val - min_val
    if range_val == 0:
        return [0.0] * len(values)
    return [(x - min_val) / range_val for x in values]

def standardize(values: List[float]) -> List[float]:
    avg = mean(values)
    sd = std(values)
    if sd == 0:
        return [0.0] * len(values)
    return [(x - avg) / sd for x in values]

# Load and use
data = load_csv('BostonHousing.csv')
print(f"Loaded {len(data)} columns")
```

**Problem**: If we want to use these functions in another notebook or script, we'd have to copy-paste everything! ğŸ˜±

---

## Part 2: Creating Our First Module - `statistics.py`

Let's organize our statistical functions into a module.

### Step 1: Create `statistics.py`

âš ï¸ **Create this file in the same directory as your notebook** âš ï¸

```python
# statistics.py
"""
Statistical functions for data analysis.

This module provides basic statistical computations including
mean, standard deviation, and variance calculations.
"""

import math
from typing import List


def mean(values: List[float]) -> float:
    """
    Calculate the arithmetic mean (average) of a list of values.
    
    Formula: mean = (sum of all values) / (number of values)
    
    Args:
        values: List of numeric values
        
    Returns:
        The arithmetic mean as a float
        
    Example:
        >>> mean([1.0, 2.0, 3.0, 4.0, 5.0])
        3.0
    """
    return sum(values) / len(values)


def std(values: List[float]) -> float:
    """
    Calculate the sample standard deviation of a list of values.
    
    Formula: std = sqrt(sum((x - mean)Â²) / (n - 1))
    
    Args:
        values: List of numeric values
        
    Returns:
        The sample standard deviation as a float
        
    Example:
        >>> std([1.0, 2.0, 3.0, 4.0, 5.0])
        1.5811388300841898
    """
    n = len(values)
    avg = mean(values)
    variance = sum((x - avg) ** 2 for x in values) / (n - 1)
    return math.sqrt(variance)


def variance(values: List[float]) -> float:
    """
    Calculate the sample variance of a list of values.
    
    Formula: variance = sum((x - mean)Â²) / (n - 1)
    
    Args:
        values: List of numeric values
        
    Returns:
        The sample variance as a float
    """
    n = len(values)
    avg = mean(values)
    return sum((x - avg) ** 2 for x in values) / (n - 1)


def describe(values: List[float]) -> dict:
    """
    Generate descriptive statistics for a list of values.
    
    Args:
        values: List of numeric values
        
    Returns:
        Dictionary containing mean, std, min, max, and count
    """
    return {
        'mean': mean(values),
        'std': std(values),
        'min': min(values),
        'max': max(values),
        'count': len(values)
    }
```

### Step 2: Test the Module

```python
# In your notebook
import statistics

# Test with simple data
test_data = [1.0, 2.0, 3.0, 4.0, 5.0]
print(f"Mean: {statistics.mean(test_data)}")
print(f"Std: {statistics.std(test_data):.4f}")
print(f"Description: {statistics.describe(test_data)}")
```

### Step 3: Make Changes and Reload

Now add a new function to `statistics.py`:

```python
def median(values: List[float]) -> float:
    """Calculate the median (middle value) of a list."""
    sorted_values = sorted(values)
    n = len(sorted_values)
    if n % 2 == 0:
        return (sorted_values[n//2 - 1] + sorted_values[n//2]) / 2
    else:
        return sorted_values[n//2]
```

**Without reloading**, try to use it:

```python
statistics.median([1, 2, 3, 4, 5])  # This will fail!
```

**Now reload the module**:

```python
import importlib
importlib.reload(statistics)

statistics.median([1, 2, 3, 4, 5])  # Now it works! Returns 3.0
```

---

## Part 3: Creating `data_loader.py` Module

### Step 1: Create `data_loader.py`

```python
# data_loader.py
"""
Data loading utilities for CSV files.

This module handles reading CSV files and converting them
into usable data structures for analysis.
"""

from typing import Dict, List


def load_csv(filename: str) -> Dict[str, List[float]]:
    """
    Load a CSV file and return a dictionary with column names as keys
    and lists of float values.
    
    Args:
        filename: Path to the CSV file
        
    Returns:
        Dictionary mapping column names to lists of float values
        
    Raises:
        FileNotFoundError: If the file doesn't exist
        ValueError: If conversion to float fails
    """
    data: Dict[str, List[float]] = {}
    
    try:
        with open(filename, 'r') as file:
            header = file.readline().strip()
            columns = header.split(',')
            
            for col in columns:
                data[col] = []
            
            for line_num, line in enumerate(file, start=2):
                values = line.strip().split(',')
                for col, value in zip(columns, values):
                    try:
                        data[col].append(float(value))
                    except ValueError:
                        raise ValueError(
                            f"Cannot convert '{value}' to float "
                            f"at line {line_num}, column '{col}'"
                        )
    except FileNotFoundError:
        raise FileNotFoundError(f"File '{filename}' not found")
    
    return data


def get_column_names(data: Dict[str, List[float]]) -> List[str]:
    """Get list of column names from the data dictionary."""
    return list(data.keys())


def get_column(data: Dict[str, List[float]], column_name: str) -> List[float]:
    """
    Get a specific column from the data.
    
    Args:
        data: Data dictionary
        column_name: Name of the column to retrieve
        
    Returns:
        List of values for the specified column
        
    Raises:
        KeyError: If column doesn't exist
    """
    if column_name not in data:
        raise KeyError(
            f"Column '{column_name}' not found. "
            f"Available columns: {', '.join(data.keys())}"
        )
    return data[column_name]


def filter_rows(data: Dict[str, List[float]], 
                column: str, 
                min_value: float = None, 
                max_value: float = None) -> Dict[str, List[float]]:
    """
    Filter data based on value range in a specific column.
    
    Args:
        data: Data dictionary
        column: Column to filter on
        min_value: Minimum value (inclusive)
        max_value: Maximum value (inclusive)
        
    Returns:
        New dictionary with filtered data
    """
    if column not in data:
        raise KeyError(f"Column '{column}' not found")
    
    filtered_data = {col: [] for col in data.keys()}
    
    for i, value in enumerate(data[column]):
        include = True
        if min_value is not None and value < min_value:
            include = False
        if max_value is not None and value > max_value:
            include = False
        
        if include:
            for col in data.keys():
                filtered_data[col].append(data[col][i])
    
    return filtered_data
```

### Step 2: Use Both Modules Together

```python
import data_loader
import statistics

# Load the data
data = data_loader.load_csv('BostonHousing.csv')

# Analyze a specific column
medv = data_loader.get_column(data, 'MEDV')
print(f"MEDV Statistics: {statistics.describe(medv)}")

# Filter expensive houses
expensive_houses = data_loader.filter_rows(data, 'MEDV', min_value=30.0)
print(f"Houses with MEDV >= 30: {len(expensive_houses['MEDV'])}")
```

---

## Part 4: Creating `normalization.py` Module

### Step 1: Create `normalization.py`

```python
# normalization.py
"""
Data normalization and scaling functions.

This module provides various methods to normalize and scale
numeric data for machine learning applications.
"""

from typing import List
import statistics  # We can import our own module!


def min_max_scale(values: List[float]) -> List[float]:
    """
    Normalize values to the [0, 1] range using Min-Max scaling.
    
    Formula: x_scaled = (x - min) / (max - min)
    
    Args:
        values: List of numeric values
        
    Returns:
        List of normalized values in range [0, 1]
    """
    min_val = min(values)
    max_val = max(values)
    range_val = max_val - min_val
    
    if range_val == 0:
        return [0.0] * len(values)
    
    return [(x - min_val) / range_val for x in values]


def standardize(values: List[float]) -> List[float]:
    """
    Standardize values using z-score normalization.
    
    Formula: z = (x - mean) / std
    
    This function reuses the mean() and std() functions
    from our statistics module!
    
    Args:
        values: List of numeric values
        
    Returns:
        List of standardized values (z-scores)
    """
    avg = statistics.mean(values)  # Using our module!
    sd = statistics.std(values)    # Using our module!
    
    if sd == 0:
        return [0.0] * len(values)
    
    return [(x - avg) / sd for x in values]


def apply_scaling(values: List[float], method: str) -> List[float]:
    """
    Apply the specified scaling method to the data.
    
    Args:
        values: List of numeric values to scale
        method: Scaling method - either "minmax" or "standard"
        
    Returns:
        List of scaled values
        
    Raises:
        ValueError: If method is not recognized
    """
    if method == "minmax":
        return min_max_scale(values)
    elif method == "standard":
        return standardize(values)
    else:
        raise ValueError(
            f"Unknown method: '{method}'. "
            f"Valid options are 'minmax' or 'standard'"
        )


def normalize_dataset(data: dict, 
                     method: str = "standard",
                     exclude_columns: List[str] = None) -> dict:
    """
    Normalize all columns in a dataset.
    
    Args:
        data: Data dictionary with column names and values
        method: Normalization method ("minmax" or "standard")
        exclude_columns: List of columns to skip (e.g., binary features)
        
    Returns:
        New dictionary with normalized values
    """
    if exclude_columns is None:
        exclude_columns = []
    
    normalized = {}
    for column, values in data.items():
        if column in exclude_columns:
            normalized[column] = values.copy()
        else:
            normalized[column] = apply_scaling(values, method)
    
    return normalized
```

### Step 2: Test the Complete Workflow

```python
import data_loader
import statistics
import normalization
import importlib

# Reload all modules (if you made changes)
importlib.reload(data_loader)
importlib.reload(statistics)
importlib.reload(normalization)

# Load data
data = data_loader.load_csv('BostonHousing.csv')
print(f"Loaded {len(data)} columns with {len(data['MEDV'])} rows")

# Analyze original data
print("\nOriginal MEDV statistics:")
print(statistics.describe(data['MEDV']))

# Normalize the entire dataset (exclude binary CHAS column)
normalized_data = normalization.normalize_dataset(
    data, 
    method="standard",
    exclude_columns=['CHAS']
)

# Analyze normalized data
print("\nNormalized MEDV statistics:")
print(statistics.describe(normalized_data['MEDV']))
```

---

## Part 5: Creating a Package Structure

Now let's organize everything into a proper package!

### Step 1: Create Package Structure

Create the following folder structure:

```
boston_analysis/
 |- __init__.py
 |- data_loader.py
 |- statistics.py
 |- normalization.py
 |- report.py
```

### Step 2: Move Files

1. Create a folder named `boston_analysis` in your notebook directory
2. Create an empty `__init__.py` file inside it
3. Move all three `.py` files into the `boston_analysis` folder

### Step 3: Create `report.py`

Inside the `boston_analysis` folder, create `report.py`:

```python
# boston_analysis/report.py
"""
Reporting utilities for data analysis.

This module generates formatted reports and summaries.
"""

from typing import Dict, List
from . import statistics  # Import from our own package!


def column_report(data: Dict[str, List[float]], column: str) -> str:
    """
    Generate a text report for a specific column.
    
    Args:
        data: Data dictionary
        column: Column name to report on
        
    Returns:
        Formatted string with statistics
    """
    if column not in data:
        return f"Column '{column}' not found!"
    
    values = data[column]
    stats = statistics.describe(values)
    
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Column: {column:<30} â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Count:  {stats['count']:<30} â•‘
â•‘  Mean:   {stats['mean']:<30.4f} â•‘
â•‘  Std:    {stats['std']:<30.4f} â•‘
â•‘  Min:    {stats['min']:<30.4f} â•‘
â•‘  Max:    {stats['max']:<30.4f} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    return report


def full_dataset_report(data: Dict[str, List[float]]) -> str:
    """Generate a report for all columns in the dataset."""
    reports = []
    for column in data.keys():
        reports.append(column_report(data, column))
    return "\n".join(reports)
```

### Step 4: Update `__init__.py`

Make it easier to import by editing `__init__.py`:

```python
# boston_analysis/__init__.py
"""
Boston Housing Analysis Package

A toolkit for loading, analyzing, and normalizing housing data.
"""

from . import data_loader
from . import statistics
from . import normalization
from . import report

__version__ = "1.0.0"
__all__ = ['data_loader', 'statistics', 'normalization', 'report']
```

### Step 5: Use the Package

**Restart your kernel**, then:

```python
# Import the entire package
import boston_analysis

# Load data using the package
data = boston_analysis.data_loader.load_csv('BostonHousing.csv')

# Generate a report
print(boston_analysis.report.column_report(data, 'MEDV'))

# Normalize data
normalized = boston_analysis.normalization.normalize_dataset(
    data,
    method="standard",
    exclude_columns=['CHAS']
)

# Show before and after
print("ORIGINAL:")
print(boston_analysis.report.column_report(data, 'MEDV'))
print("\nNORMALIZED:")
print(boston_analysis.report.column_report(normalized, 'MEDV'))
```

**Alternative import styles**:

```python
# Import specific modules
from boston_analysis import statistics, normalization

medv_values = data['MEDV']
print(f"Mean: {statistics.mean(medv_values)}")
scaled = normalization.min_max_scale(medv_values)
```

```python
# Import specific functions
from boston_analysis.statistics import mean, std
from boston_analysis.normalization import standardize

print(f"Mean: {mean(data['MEDV'])}")
print(f"Std: {std(data['MEDV'])}")
```

---

## Part 6: Advanced Package Features

### Step 6a: Create Subpackages

Expand the package structure:

```
boston_analysis/
 |- __init__.py
 |- data_loader.py
 |- report.py
 |
 |- stats/
 |    |- __init__.py
 |    |- descriptive.py  (mean, std, median)
 |    |- inferential.py  (correlation, t-test)
 |
 |- preprocessing/
 |    |- __init__.py
 |    |- scaling.py      (min_max_scale, standardize)
 |    |- transformation.py (log, sqrt transforms)
```

### Step 6b: Create `stats/descriptive.py`

```python
# boston_analysis/stats/descriptive.py
"""Descriptive statistics functions."""

import math
from typing import List


def mean(values: List[float]) -> float:
    """Calculate mean."""
    return sum(values) / len(values)


def std(values: List[float]) -> float:
    """Calculate standard deviation."""
    n = len(values)
    avg = mean(values)
    variance = sum((x - avg) ** 2 for x in values) / (n - 1)
    return math.sqrt(variance)


def median(values: List[float]) -> float:
    """Calculate median."""
    sorted_values = sorted(values)
    n = len(sorted_values)
    if n % 2 == 0:
        return (sorted_values[n//2 - 1] + sorted_values[n//2]) / 2
    return sorted_values[n//2]


def quartiles(values: List[float]) -> tuple:
    """Calculate Q1, Q2 (median), Q3."""
    sorted_values = sorted(values)
    n = len(sorted_values)
    q2 = median(sorted_values)
    q1 = median(sorted_values[:n//2])
    q3 = median(sorted_values[(n+1)//2:])
    return q1, q2, q3
```

### Step 6c: Use Subpackages

```python
import boston_analysis.stats.descriptive as stats

values = data['MEDV']
q1, q2, q3 = stats.quartiles(values)
print(f"Q1: {q1:.2f}, Median: {q2:.2f}, Q3: {q3:.2f}")
```

---

## Part 7: Practical Exercise

**Challenge**: Complete the package by adding these features:

1. **In `preprocessing/transformation.py`**: Add functions for:
   - `log_transform(values)` - Apply log transformation
   - `sqrt_transform(values)` - Apply square root transformation
   - `box_cox_transform(values, lambda_param)` - Box-Cox transformation

2. **In `stats/inferential.py`**: Add:
   - `correlation(x, y)` - Calculate Pearson correlation
   - `covariance(x, y)` - Calculate covariance

3. **Create a complete analysis script** that:
   - Loads the Boston Housing data
   - Generates reports for all columns
   - Normalizes the data
   - Computes correlations between features
   - Saves results to a file

### Example Solution Structure:

```python
# analysis_script.py
"""Complete analysis of Boston Housing dataset."""

from boston_analysis import data_loader, report
from boston_analysis.stats import descriptive
from boston_analysis.preprocessing import scaling

# Load data
print("Loading data...")
data = data_loader.load_csv('BostonHousing.csv')

# Generate reports
print("\n" + "="*50)
print("DESCRIPTIVE STATISTICS")
print("="*50)
print(report.full_dataset_report(data))

# Normalize
print("\nNormalizing data...")
normalized = scaling.normalize_dataset(data, method="standard")

# Analyze specific relationships
print("\nAnalyzing RM (rooms) vs MEDV (price)...")
rm_mean = descriptive.mean(data['RM'])
medv_mean = descriptive.mean(data['MEDV'])
print(f"Average rooms: {rm_mean:.2f}")
print(f"Average price: ${medv_mean:.1f}k")
```

---

## Summary

You've learned to:

âœ… **Organize code into modules** for reusability  
âœ… **Create Python packages** with proper structure  
âœ… **Use import and reload** for development  
âœ… **Apply type hints** for better code quality  
âœ… **Build subpackages** for complex projects  
âœ… **Create a practical data analysis toolkit**

**Key Takeaways**:
- Modules = single `.py` files with related functions
- Packages = folders with `__init__.py` containing multiple modules
- Good organization makes code reusable across projects
- Type hints help catch errors and document code
- Reload modules during development to see changes

**Next Steps**:
- Add unit tests for your package
- Create documentation with docstrings
- Publish your package to PyPI
- Use your package in multiple analysis notebooks!